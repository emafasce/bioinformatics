{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"make_dataset.ipynb","provenance":[{"file_id":"15zMuAu4tBo5EEvzgSSUbKeL22fsgAXgB","timestamp":1633194620176}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttFQ3W0bJUTL","executionInfo":{"status":"ok","timestamp":1633244877344,"user_tz":-120,"elapsed":84001,"user":{"displayName":"Matteo Nole","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13157484582686084266"}},"outputId":"ee75b49f-3a83-46ac-eb6a-667b804397e8"},"source":["def connect_to_drive():\n","  from google.colab import drive\n","  drive.mount('/content/gdrive', force_remount=True)\n","\n","!pip install import_ipynb \n","import import_ipynb  \n","connect_to_drive()\n","%cd '/content/gdrive/My Drive/Bioinformatics2'"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting import_ipynb\n","  Downloading import-ipynb-0.1.3.tar.gz (4.0 kB)\n","Building wheels for collected packages: import-ipynb\n","  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-py3-none-any.whl size=2975 sha256=c8c0761c15242f5a39083a656b626569882bdb571a9ce54e97bcfc93b5bd8b41\n","  Stored in directory: /root/.cache/pip/wheels/b1/5e/dc/79780689896a056199b0b9f24471e3ee184fbd816df355d5f0\n","Successfully built import-ipynb\n","Installing collected packages: import-ipynb\n","Successfully installed import-ipynb-0.1.3\n","Mounted at /content/gdrive\n","/content/gdrive/My Drive/Bioinformatics2\n"]}]},{"cell_type":"code","metadata":{"id":"BJIUX5xC2eH1","executionInfo":{"status":"ok","timestamp":1633244967775,"user_tz":-120,"elapsed":3777,"user":{"displayName":"Matteo Nole","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13157484582686084266"}}},"source":["from pathlib import Path\n","import os\n","import cv2\n","import argparse\n","import albumentations.augmentations.functional as F\n","from utils import *\n","import torch\n","import cv2"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"lDOCli1NOQKm","executionInfo":{"status":"ok","timestamp":1633244975728,"user_tz":-120,"elapsed":291,"user":{"displayName":"Matteo Nole","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13157484582686084266"}}},"source":["def crop_images(img_list, output_path, txt_cropped_inputs_path, txt_cropped_labels_path):\n","\n","    Path(out_path).mkdir(parents=True, exist_ok=True)\n","    for idx, img_path in enumerate(img_list):\n","\n","        image=cv2.imread(str(img_path))\n","        seg_path=str(img_path).replace('crop', 'seg_crop')\n","        seg = cv2.imread(seg_path)\n","        if seg is None:\n","          seg_path=seg_path.replace('seg_crop','segmentation_crop')\n","          seg = cv2.imread(seg_path)\n","          if seg is None:\n","            seg_path=seg_path.replace('segmentation_crop','segmentation-crop')\n","            seg = cv2.imread(seg_path)\n","\n","        #image = F.pad(image, min_height=IMG_DIM, min_width=IMG_DIM)\n","        #seg = F.pad(seg, min_height=IMG_DIM, min_width=IMG_DIM)\n","        \n","\n","        tile = 0\n","        for i in range(0, 2000, SLICE_LEN):\n","            for j in range(0, 2000, SLICE_LEN):\n","                tile += 1\n","                path_img=os.path.join(out_path,str(img_path).split('/')[-1].split('.')[0]+f\"_{tile:02}.png\")\n","                path_seg=os.path.join(out_path, str(seg_path).split('/')[-1].split('.')[0]+f\"_{tile:02}.png\")\n","                print(path_img)\n","                print(path_seg)\n","                cv2.imwrite(path_img,\n","                            image[i:i + INPUT_DIM, j:j + INPUT_DIM])\n","                cv2.imwrite(path_seg,\n","                            seg[i:i + INPUT_DIM, j:j + INPUT_DIM])\n","\n","def crop_squared_tiles_from_image(image, slice_length):\n","  '''\n","  It takes a PIL Image and returns an ordered list of cropped squared PIL Images of side slice_length from the original image. \n","  Make sure that slice_length is divisible for the image length and width. \n","  '''\n","  squared_tiles=[]\n","  image_width, image_height, image_depth = image.shape\n","  for i in range(0, image_width, slice_length):\n","    for j in range(0, image_height, slice_length):\n","      squared_tiles.append(image[i:i + slice_length, j:j + slice_length])\n","  return squared_tiles\n","\n","def save_crop_squared_tiles_from_path_list(img_path_list, output_folder, txt_file_path, slice_length):\n","  '''\n","  It takes a list of images paths, crops every image of the list into squared tiles of sides slice_length, saves them into output \n","  folder and saves their name into txt_file_path\n","  '''\n","  path_tiles=[]\n","  for img_path in img_path_list:\n","    image=cv2.imread(img_path)\n","    squared_tiles=crop_squared_tiles_from_image(image, slice_length)\n","    for tile_number,tile in enumerate(squared_tiles):\n","      path_tile=os.path.join(output_folder,str(img_path).split('/')[-1].split('.')[0]+f\"_{tile_number:02}.png\")\n","      path_tile=str(path_tile)\n","      path_tiles.append(path_tile)\n","      cv2.imwrite(path_tile, tile)\n","  save_paths_images(path_tiles, txt_file_path)\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"tBjNdwSW55r3","executionInfo":{"status":"ok","timestamp":1633245183242,"user_tz":-120,"elapsed":101130,"user":{"displayName":"Matteo Nole","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13157484582686084266"}}},"source":["test_input_path='/content/gdrive/MyDrive/Bioinformatics2/dataset_paths/validation_paths_inputs.txt'\n","test_target_path='/content/gdrive/MyDrive/Bioinformatics2/dataset_paths/validation_paths_targets.txt'\n","test_input_list=read_list_images_path_from_txt(test_input_path)\n","test_target_list=read_list_images_path_from_txt(test_target_path)\n","\n","slice_length=1000\n","output_folder = '/content/gdrive/MyDrive/Bioinformatics2/validation_dataset_1000/'\n","txt_file_input_path = '/content/gdrive/MyDrive/Bioinformatics2/dataset_paths/validation_paths_inputs_1000.txt'\n","txt_file_target_path = '/content/gdrive/MyDrive/Bioinformatics2/dataset_paths/validation_paths_targets_1000.txt'\n","\n","save_crop_squared_tiles_from_path_list(test_input_list, output_folder, txt_file_input_path, slice_length)\n","save_crop_squared_tiles_from_path_list(test_target_list, output_folder, txt_file_target_path, slice_length)"],"execution_count":6,"outputs":[]}]}